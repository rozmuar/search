# –ê–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞

## –û–±–∑–æ—Ä

–ü–æ–∏—Å–∫–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   –ó–∞–ø—Ä–æ—Å    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ –û–±—Ä–∞–±–æ—Ç–∫–∞   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   –ü–æ–∏—Å–∫     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ –†–∞–Ω–∂–∏—Ä–æ-    ‚îÇ
‚îÇ             ‚îÇ    ‚îÇ   –∑–∞–ø—Ä–æ—Å–∞   ‚îÇ    ‚îÇ             ‚îÇ    ‚îÇ   –≤–∞–Ω–∏–µ     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                               ‚îÇ
                                                               ‚ñº
                                                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                         ‚îÇ  –†–µ–∑—É–ª—å—Ç–∞—Ç  ‚îÇ
                                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (Query Processing)

### 1.1 –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è

```python
def normalize_query(query: str) -> str:
    """
    –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ
    """
    # 1. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    query = query.lower()
    
    # 2. –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    query = ' '.join(query.split())
    
    # 3. –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–∫—Ä–æ–º–µ –¥–µ—Ñ–∏—Å–∞)
    query = re.sub(r'[^\w\s\-]', '', query)
    
    # 4. –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    # "–∞–π—Ñ–æ–Ω" -> —Ç–∞–∫–∂–µ –∏—Å–∫–∞—Ç—å "iphone"
    query_translit = transliterate(query)
    
    return query, query_translit
```

### 1.2 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è

```python
def tokenize(query: str) -> List[str]:
    """
    –†–∞–∑–±–∏–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã
    """
    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º
    tokens = query.split()
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    tokens = [t for t in tokens if t not in STOPWORDS]
    
    # –°—Ç–µ–º–º–∏–Ω–≥/–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    tokens = [stem(t) for t in tokens]
    
    return tokens

# –ü—Ä–∏–º–µ—Ä:
# "–∫—Ä–∞—Å–Ω—ã–µ –∫—Ä–æ—Å—Å–æ–≤–∫–∏ nike" -> ["–∫—Ä–∞—Å–Ω", "–∫—Ä–æ—Å—Å–æ–≤–∫", "nike"]
```

### 1.3 –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫

```python
def fix_typos(tokens: List[str], dictionary: Set[str]) -> List[str]:
    """
    –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫ —Å –ø–æ–º–æ—â—å—é —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞
    """
    fixed = []
    for token in tokens:
        if token in dictionary:
            fixed.append(token)
        else:
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–µ–µ —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–∞—Ä–µ
            candidates = []
            for word in dictionary:
                distance = levenshtein(token, word)
                if distance <= 2:  # –º–∞–∫—Å–∏–º—É–º 2 –æ—à–∏–±–∫–∏
                    candidates.append((word, distance))
            
            if candidates:
                # –ë–µ—Ä—ë–º —Å–ª–æ–≤–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º
                best = min(candidates, key=lambda x: x[1])
                fixed.append(best[0])
            else:
                fixed.append(token)
    
    return fixed

# –ü—Ä–∏–º–µ—Ä:
# "–∫—Ä–∞—Å–Ω—ã–π–µ –∫—Ä–æ—Å–æ–≤–∫–∏" -> "–∫—Ä–∞—Å–Ω—ã–µ –∫—Ä–æ—Å—Å–æ–≤–∫–∏"
```

### 1.4 –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏

```python
def expand_synonyms(tokens: List[str], synonyms: Dict) -> List[List[str]]:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    """
    expanded = []
    for token in tokens:
        if token in synonyms:
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω + –≤—Å–µ –µ–≥–æ —Å–∏–Ω–æ–Ω–∏–º—ã
            expanded.append([token] + synonyms[token])
        else:
            expanded.append([token])
    
    return expanded

# –ü—Ä–∏–º–µ—Ä:
# synonyms = {"—Ç–µ–ª–µ—Ñ–æ–Ω": ["—Å–º–∞—Ä—Ç—Ñ–æ–Ω", "–º–æ–±–∏–ª—å–Ω—ã–π"]}
# "—Ç–µ–ª–µ—Ñ–æ–Ω samsung" -> [["—Ç–µ–ª–µ—Ñ–æ–Ω", "—Å–º–∞—Ä—Ç—Ñ–æ–Ω", "–º–æ–±–∏–ª—å–Ω—ã–π"], ["samsung"]]
```

## 2. –ü–æ–∏—Å–∫ (Search)

### 2.1 –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å

–û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞:

```
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
"–∫—Ä–æ—Å—Å–æ–≤–∫–∏" -> {product_1: 0.95, product_5: 0.87, product_12: 0.82}
"nike"      -> {product_1: 0.90, product_3: 0.85}
"–∫—Ä–∞—Å–Ω—ã–π"   -> {product_1: 0.80, product_7: 0.75}

–ü–æ–∏—Å–∫ "–∫—Ä–∞—Å–Ω—ã–µ –∫—Ä–æ—Å—Å–æ–≤–∫–∏ nike":
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. –ü–æ–ª—É—á–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
2. –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ (AND) –∏–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ (OR)
3. –°—É–º–º–∏—Ä—É–µ–º/—É—Å—Ä–µ–¥–Ω—è–µ–º —Å–∫–æ—Ä—ã
```

```python
def search_inverted_index(tokens: List[str], index: Dict) -> Dict[str, float]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É
    """
    results = {}
    
    for token in tokens:
        if token in index:
            for product_id, score in index[token].items():
                if product_id in results:
                    results[product_id] += score
                else:
                    results[product_id] = score
    
    return results
```

### 2.2 N-gram –∏–Ω–¥–µ–∫—Å –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è

–î–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —á–∞—Å—Ç–∏ —Å–ª–æ–≤–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º n-–≥—Ä–∞–º–º—ã:

```
–°–ª–æ–≤–æ "–∫—Ä–æ—Å—Å–æ–≤–∫–∏" —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Ç—Ä–∏–≥—Ä–∞–º–º—ã:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
"–∫—Ä–æ", "—Ä–æ—Å", "–æ—Å—Å", "—Å—Å–æ", "—Å–æ–≤", "–æ–≤–∫", "–≤–∫–∏"

–ü—Ä–∏ –ø–æ–∏—Å–∫–µ "–∫—Ä–æ—Å—Å" –Ω–∞—Ö–æ–¥–∏–º —Ç—Ä–∏–≥—Ä–∞–º–º—ã:
"–∫—Ä–æ", "—Ä–æ—Å", "–æ—Å—Å", "—Å—Å–æ"

–ò—â–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å –∏–Ω–¥–µ–∫—Å–æ–º -> –Ω–∞—Ö–æ–¥–∏–º "–∫—Ä–æ—Å—Å–æ–≤–∫–∏"
```

```python
def generate_ngrams(word: str, n: int = 3) -> List[str]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è n-–≥—Ä–∞–º–º –¥–ª—è —Å–ª–æ–≤–∞
    """
    # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞
    word = f"__{word}__"
    return [word[i:i+n] for i in range(len(word) - n + 1)]

def search_ngram_index(partial: str, ngram_index: Dict) -> List[str]:
    """
    –ü–æ–∏—Å–∫ —Å–ª–æ–≤ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
    """
    ngrams = generate_ngrams(partial)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
    word_scores = {}
    for ngram in ngrams:
        if ngram in ngram_index:
            for word in ngram_index[ngram]:
                word_scores[word] = word_scores.get(word, 0) + 1
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É n-–≥—Ä–∞–º–º
    for word in word_scores:
        word_ngrams = len(generate_ngrams(word))
        word_scores[word] /= word_ngrams
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    return sorted(word_scores.items(), key=lambda x: -x[1])
```

### 2.3 Prefix-–¥–µ—Ä–µ–≤–æ –¥–ª—è –ø–æ–¥—Å–∫–∞–∑–æ–∫

–î–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º Trie:

```
                    root
                   /    \
                  –∫      n
                 /        \
                —Ä          i
               /            \
              –æ              k
             /                \
            —Å     "–∫—Ä–æ—Å..."    e    "nike"
           /
          —Å
         /
        –æ
       /
      –≤
     /
    –∫
   /
  –∏     "–∫—Ä–æ—Å—Å–æ–≤–∫–∏"
```

```python
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_end = False
        self.products = []  # —Ç–æ–≤–∞—Ä—ã –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞
        self.popularity = 0  # –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞

class SuggestionTrie:
    def __init__(self):
        self.root = TrieNode()
    
    def insert(self, word: str, product_ids: List[str], popularity: int = 0):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.is_end = True
        node.products = product_ids
        node.popularity = popularity
    
    def search_prefix(self, prefix: str, limit: int = 10) -> List[Suggestion]:
        """
        –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Å–ª–æ–≤ —Å –¥–∞–Ω–Ω—ã–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
        """
        node = self.root
        for char in prefix:
            if char not in node.children:
                return []
            node = node.children[char]
        
        # DFS –¥–ª—è —Å–±–æ—Ä–∞ –≤—Å–µ—Ö —Å–ª–æ–≤
        suggestions = []
        self._collect_words(node, prefix, suggestions)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
        suggestions.sort(key=lambda x: -x.popularity)
        return suggestions[:limit]
```

## 3. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (Ranking)

### 3.1 –§–∞–∫—Ç–æ—Ä—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è

```python
@dataclass
class RankingFactors:
    # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
    text_match_score: float      # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –∑–∞–ø—Ä–æ—Å–æ–º
    position_score: float        # –ü–æ–∑–∏—Ü–∏—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–Ω–∞—á–∞–ª–æ –≤–∞–∂–Ω–µ–µ)
    exact_match_bonus: float     # –ë–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    
    # –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
    in_stock: bool               # –ù–∞–ª–∏—á–∏–µ –Ω–∞ —Å–∫–ª–∞–¥–µ
    price_score: float           # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞
    discount_score: float        # –†–∞–∑–º–µ—Ä —Å–∫–∏–¥–∫–∏
    
    # –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å
    sales_count: int             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥–∞–∂
    views_count: int             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
    click_rate: float            # CTR –≤ –ø–æ–∏—Å–∫–µ
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ
    freshness: float             # –ù–æ–≤–∏–∑–Ω–∞ —Ç–æ–≤–∞—Ä–∞
    image_quality: float         # –ù–∞–ª–∏—á–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ñ–æ—Ç–æ
```

### 3.2 –§–æ—Ä–º—É–ª–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è

```python
def calculate_score(product: Product, query: str, factors: RankingFactors) -> float:
    """
    –†–∞—Å—á—ë—Ç –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å–∫–æ—Ä–∞ —Ç–æ–≤–∞—Ä–∞
    """
    
    # –í–µ—Å–∞ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ)
    W_TEXT = 0.4
    W_STOCK = 0.2
    W_POPULARITY = 0.2
    W_COMMERCIAL = 0.2
    
    # 1. –¢–µ–∫—Å—Ç–æ–≤–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
    text_score = (
        factors.text_match_score * 0.5 +
        factors.position_score * 0.3 +
        factors.exact_match_bonus * 0.2
    )
    
    # 2. –ù–∞–ª–∏—á–∏–µ (–±–∏–Ω–∞—Ä–Ω—ã–π –±—É—Å—Ç)
    stock_score = 1.0 if factors.in_stock else 0.3
    
    # 3. –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å
    popularity_score = normalize(
        factors.sales_count * 0.4 +
        factors.views_count * 0.3 +
        factors.click_rate * 0.3
    )
    
    # 4. –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
    commercial_score = (
        factors.discount_score * 0.6 +
        (1 - factors.price_score) * 0.4  # –¥–µ—à–µ–≤–ª–µ = –ª—É—á—à–µ
    )
    
    # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä
    final_score = (
        text_score * W_TEXT +
        stock_score * W_STOCK +
        popularity_score * W_POPULARITY +
        commercial_score * W_COMMERCIAL
    )
    
    return final_score
```

### 3.3 –¢–µ–∫—Å—Ç–æ–≤–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (BM25)

```python
def bm25_score(query_tokens: List[str], document: str, 
               avg_doc_length: float, doc_count: int,
               k1: float = 1.5, b: float = 0.75) -> float:
    """
    BM25 - –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    """
    doc_tokens = tokenize(document)
    doc_length = len(doc_tokens)
    score = 0.0
    
    for token in query_tokens:
        # –ß–∞—Å—Ç–æ—Ç–∞ —Ç–æ–∫–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        tf = doc_tokens.count(token)
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ç–æ–∫–µ–Ω–æ–º
        df = get_document_frequency(token)
        
        # IDF - –æ–±—Ä–∞—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞
        idf = math.log((doc_count - df + 0.5) / (df + 0.5))
        
        # BM25 —Ñ–æ—Ä–º—É–ª–∞
        numerator = tf * (k1 + 1)
        denominator = tf + k1 * (1 - b + b * doc_length / avg_doc_length)
        
        score += idf * (numerator / denominator)
    
    return score
```

## 4. –ü–æ–¥—Å–∫–∞–∑–∫–∏ (Suggestions)

### 4.1 –¢–∏–ø—ã –ø–æ–¥—Å–∫–∞–∑–æ–∫

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –ü–æ–ª–µ –≤–≤–æ–¥–∞: "–∫—Ä–∞—Å–Ω"                                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  üìù –ü–æ–∏—Å–∫–æ–≤—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏:                                    ‚îÇ
‚îÇ     ‚Ä¢ –∫—Ä–∞—Å–Ω—ã–µ –∫—Ä–æ—Å—Å–æ–≤–∫–∏                                     ‚îÇ
‚îÇ     ‚Ä¢ –∫—Ä–∞—Å–Ω–∞—è –∫—É—Ä—Ç–∫–∞                                        ‚îÇ
‚îÇ     ‚Ä¢ –∫—Ä–∞—Å–Ω–æ–µ –ø–ª–∞—Ç—å–µ                                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  üè∑Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:                                              ‚îÇ
‚îÇ     ‚Ä¢ –ö—Ä–∞—Å–Ω–∞—è –æ–±—É–≤—å (234)                                   ‚îÇ
‚îÇ     ‚Ä¢ –ö—Ä–∞—Å–Ω–∞—è –æ–¥–µ–∂–¥–∞ (567)                                  ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  üì¶ –¢–æ–≤–∞—Ä—ã:                                                  ‚îÇ
‚îÇ     ‚Ä¢ [img] –ö—Ä–æ—Å—Å–æ–≤–∫–∏ Nike Air Red     2 990 ‚ÇΩ             ‚îÇ
‚îÇ     ‚Ä¢ [img] –ö—É—Ä—Ç–∫–∞ –∫—Ä–∞—Å–Ω–∞—è –∑–∏–º–Ω—è—è      5 490 ‚ÇΩ             ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.2 –ê–ª–≥–æ—Ä–∏—Ç–º –ø–æ–¥—Å–∫–∞–∑–æ–∫

```python
def get_suggestions(prefix: str, project_id: str, limit: int = 10) -> Suggestions:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è –ø—Ä–µ—Ñ–∏–∫—Å–∞
    """
    
    # 1. –ü–æ–∏—Å–∫ –≤ Trie –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É
    query_suggestions = trie.search_prefix(prefix, limit=5)
    
    # 2. –ü–æ–∏—Å–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    category_suggestions = search_categories(prefix, limit=3)
    
    # 3. –ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ (–±—ã—Å—Ç—Ä—ã–π, –ø–æ n-gram –∏–Ω–¥–µ–∫—Å—É)
    products = search_products_fast(prefix, limit=4)
    
    # 4. –£—á—ë—Ç –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
    personalized = personalize_suggestions(
        query_suggestions, 
        user_history
    )
    
    return Suggestions(
        queries=personalized,
        categories=category_suggestions,
        products=products
    )
```

### 4.3 –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥—Å–∫–∞–∑–æ–∫

```python
def personalize_suggestions(suggestions: List[str], 
                           user_history: UserHistory) -> List[str]:
    """
    –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–º–æ—Ç—Ä–µ–ª
    preferred_categories = user_history.viewed_categories
    
    # –ë—É—Å—Ç –ø–æ–¥—Å–∫–∞–∑–æ–∫ –∏–∑ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    scored = []
    for suggestion in suggestions:
        score = suggestion.base_score
        
        # –ë—É—Å—Ç –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        if suggestion.category in preferred_categories:
            score *= 1.5
        
        # –ë—É—Å—Ç –∑–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ –∑–∞–ø—Ä–æ—Å—ã
        if suggestion.text in user_history.recent_queries:
            score *= 1.3
        
        scored.append((suggestion, score))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
    scored.sort(key=lambda x: -x[1])
    return [s[0] for s in scored]
```

## 5. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### 5.1 –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
class SearchCache:
    """
    –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    """
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.local_cache = LRUCache(maxsize=1000)  # L1 - –ø–∞–º—è—Ç—å
    
    def get(self, project_id: str, query: str) -> Optional[SearchResult]:
        cache_key = f"search:{project_id}:{hash(query)}"
        
        # L1 - –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à (< 1ms)
        if cache_key in self.local_cache:
            return self.local_cache[cache_key]
        
        # L2 - Redis (< 5ms)
        cached = self.redis.get(cache_key)
        if cached:
            result = deserialize(cached)
            self.local_cache[cache_key] = result
            return result
        
        return None
    
    def set(self, project_id: str, query: str, result: SearchResult):
        cache_key = f"search:{project_id}:{hash(query)}"
        
        # –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
        ttl = 300 if self.is_popular(query) else 60
        
        self.local_cache[cache_key] = result
        self.redis.setex(cache_key, ttl, serialize(result))
```

### 5.2 –ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫—ç—à–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∏–¥–∞

```python
def invalidate_cache_on_feed_update(project_id: str, updated_products: List[str]):
    """
    –£–º–Ω–∞—è –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫—ç—à–∞
    """
    
    # –î–ª—è delta-—Ñ–∏–¥–∞: –∏–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å—ã —Å –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏
    if is_delta_update:
        for product_id in updated_products:
            # –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø—Ä–æ—Å—ã, –≥–¥–µ –±—ã–ª —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä
            affected_queries = get_queries_with_product(project_id, product_id)
            for query in affected_queries:
                cache.delete(project_id, query)
    
    # –î–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ñ–∏–¥–∞: –∏–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –≤–µ—Å—å –∫—ç—à –ø—Ä–æ–µ–∫—Ç–∞
    else:
        cache.delete_pattern(f"search:{project_id}:*")
```

### 5.3 –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

```python
async def precompute_popular_queries(project_id: str):
    """
    –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    """
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-100 –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
    popular_queries = analytics.get_popular_queries(
        project_id, 
        period='24h', 
        limit=100
    )
    
    for query in popular_queries:
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –∏ –∫—ç—à–∏—Ä—É–µ–º —Å –¥–ª–∏–Ω–Ω—ã–º TTL
        result = search_engine.search(project_id, query)
        cache.set(project_id, query, result, ttl=3600)
```

## 6. –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞

```python
@dataclass
class SearchMetrics:
    # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
    precision_at_k: float     # –¢–æ—á–Ω–æ—Å—Ç—å –≤ —Ç–æ–ø-K
    recall: float             # –ü–æ–ª–Ω–æ—Ç–∞
    ndcg: float               # Normalized DCG
    
    # –ë–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏
    click_through_rate: float # CTR
    conversion_rate: float    # –ö–æ–Ω–≤–µ—Ä—Å–∏—è
    zero_results_rate: float  # –î–æ–ª—è –ø—É—Å—Ç—ã—Ö –≤—ã–¥–∞—á
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ
    avg_response_time: float  # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
    p95_response_time: float  # 95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å
    cache_hit_rate: float     # –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫—ç—à
```
